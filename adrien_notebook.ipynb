{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-11 16:07:22.351715: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-11 16:07:22.392906: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-11 16:07:22.392941: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-11 16:07:22.393969: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-11 16:07:22.400526: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-11 16:07:22.401734: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-11 16:07:23.488596: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from google.cloud import storage\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
    "\n",
    "# Variables\n",
    "CUR_DIR = os.getcwd()\n",
    "DATA_DIR = os.path.join(CUR_DIR, \"data\")\n",
    "METADATA_DIR = os.path.join(DATA_DIR, \"metadata\")\n",
    "\n",
    "RAW_DATA = os.path.join(DATA_DIR, \"raw_data\")\n",
    "DATA_CLEAN = os.path.join(DATA_DIR, \"data_clean\")\n",
    "\n",
    "BUCKET_NAME = \"mammo_data\"\n",
    "METADATA = pd.read_csv(os.path.join(METADATA_DIR, \"train.csv\"))\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54706"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(METADATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload files to GCP bucket storage\n",
    "def upload_files_to_gcp(bucket_name: str, source_directory: str):\n",
    "    \"\"\"\n",
    "    Uploads all files from a local directory to a GCP bucket.\n",
    "\n",
    "    Parameters:\n",
    "    - bucket_name: Name of the GCP bucket.\n",
    "    - source_directory: Local directory from which to upload files.\n",
    "\n",
    "    Return:\n",
    "    - None\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize GCP Storage client\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "\n",
    "    # Ensure the destination blob folder path ends with '/'\n",
    "    if destination_blob_folder and not destination_blob_folder.endswith(\"/\"):\n",
    "        destination_blob_folder += \"/\"\n",
    "\n",
    "    # Walk through the source directory\n",
    "    for root, dirs, files in os.walk(source_directory):\n",
    "        for filename in files:\n",
    "            # Construct the local file path\n",
    "            local_path = os.path.join(root, filename)\n",
    "            # print(local_path)\n",
    "\n",
    "            # Construct the destination path in the bucket\n",
    "            if destination_blob_folder:\n",
    "                relative_path = os.path.relpath(local_path, source_directory)\n",
    "                blob_path = destination_blob_folder + relative_path\n",
    "            else:\n",
    "                blob_path = filename\n",
    "\n",
    "            # Upload the file\n",
    "            blob = bucket.blob(blob_path)\n",
    "            blob.upload_from_filename(local_path)\n",
    "            print(f'Uploaded {local_path} to \"gs://{bucket_name}/{blob_path}\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_upload_merged_csv(\n",
    "    bucket_name: str,\n",
    "    metadata_csv,\n",
    "    output_csv_name: str = \"ready_to_train.csv\",\n",
    "    file_extension: str = \".jpg\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Fetches files with a specific extension from a GCP bucket, merges their paths with another DataFrame,\n",
    "    and uploads the merged DataFrame as a CSV to the bucket.\n",
    "\n",
    "    Parameters:\n",
    "    - bucket_name: The name of the GCP bucket.\n",
    "    - metadata_csv: The DataFrame to merge with. It should have columns 'id' and 'label'.\n",
    "    - output_csv_name: The name of the output CSV file to be stored in the bucket.\n",
    "    - file_extension: The file extension to filter by. Default is '.jpg'.\n",
    "\n",
    "    Return:\n",
    "    - None\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize a GCP Storage client\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "\n",
    "    # Create a list to hold file information\n",
    "    files_info = []\n",
    "\n",
    "    # Iterate over the files in the bucket, filtering by the specified extension\n",
    "    for blob in bucket.list_blobs():\n",
    "        if blob.name.lower().endswith(file_extension):\n",
    "            file_id = blob.name.rsplit(\".\", 1)[0]  # Extract the file ID\n",
    "            files_info.append(\n",
    "                {\n",
    "                    \"image_id\": np.int64(int(file_id)),\n",
    "                    \"path\": f\"gs://{bucket_name}/{blob.name}\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # Create a DataFrame from the file information\n",
    "    df_files = pd.DataFrame(files_info)\n",
    "\n",
    "    # Select only the columnes we need\n",
    "    metadata_csv = metadata_csv[[\"image_id\", \"cancer\"]]\n",
    "\n",
    "    # Merge the DataFrames on the 'id' column\n",
    "    merged_df = pd.merge(\n",
    "        df_files, metadata_csv, on=\"image_id\", how=\"inner\"\n",
    "    )  # Use for the final CSV\n",
    "    merged_df = pd.merge(\n",
    "        df_files, metadata_csv, on=\"image_id\", how=\"left\"\n",
    "    )  # Use for the tests\n",
    "\n",
    "    # Convert the DataFrame to a CSV string\n",
    "    csv_string = merged_df.to_csv(index=False)\n",
    "\n",
    "    # Save the CSV string to a file in the bucket\n",
    "    blob = bucket.blob(output_csv_name)\n",
    "    blob.upload_from_string(csv_string, \"text/csv\")\n",
    "    print(f'CSV file \"{output_csv_name}\" uploaded to bucket \"{bucket_name}\".')\n",
    "\n",
    "\n",
    "#####\n",
    "# Dataset Creation\n",
    "#####\n",
    "# Load and process images\n",
    "def load_and_process_image(file_path: str, label):\n",
    "    \"\"\"\n",
    "    Loads and processes an image file for model training.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: The path to the image file.\n",
    "    - label: The label associated with the image file.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple containing the processed image and its label.\n",
    "    \"\"\"\n",
    "\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = tf.io.decode_jpeg(img, channels=1)\n",
    "    img = tf.image.resize(img, [128, 128])  # Resize images\n",
    "    img = img / 255.0  # Normalize to [0,1]\n",
    "    return img, label\n",
    "\n",
    "\n",
    "def create_dataset(input: str = \"local\"):\n",
    "    \"\"\"\n",
    "    Creates a dataset for model training.\n",
    "\n",
    "    Parameters:\n",
    "    - input: Specifies the source of the dataset, 'local' or 'cloud'.\n",
    "\n",
    "    Returns:\n",
    "    - TensorFlow dataset object.\n",
    "    \"\"\"\n",
    "    # local or cloud\n",
    "    # Load the dataset\n",
    "    if input == \"local\":\n",
    "        df = pd.read_csv(os.path.join(METADATA_DIR, 'ready_to_train.csv'))\n",
    "    if input == \"cloud\":\n",
    "        df = pd.read_csv(\"gs://mammo_data/ready_to_train.csv\")\n",
    "\n",
    "    # Create a TensorFlow dataset\n",
    "    paths = df[\"path\"].values\n",
    "    labels = df[\"cancer\"].values\n",
    "\n",
    "    labels = tf.cast(labels, dtype=tf.int32)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
    "    dataset = dataset.map(load_and_process_image)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "#####\n",
    "# Model\n",
    "#####\n",
    "def initialize_model():\n",
    "    \"\"\"\n",
    "    Initializes a sequential model for binary classification.\n",
    "\n",
    "    Returns:\n",
    "    - TensorFlow Sequential model.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(16, (4, 4), input_shape=(128, 128, 1), activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(32, (3, 3), activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=10, activation=\"relu\"))\n",
    "    model.add(Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "#####\n",
    "# Callback\n",
    "#####\n",
    "class custom_callback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if logs[\"accuracy\"] >= 0.97:\n",
    "            self.model.stop_training = True\n",
    "\n",
    "\n",
    "custom_callback = custom_callback()\n",
    "\n",
    "#####\n",
    "# Optimizer\n",
    "#####\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(\n",
    "    learning_rate=0.000001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Adam\"\n",
    ")\n",
    "\n",
    "#####\n",
    "# Loss Fn\n",
    "#####\n",
    "lossfn = tf.keras.losses.BinaryCrossentropy(\n",
    "    from_logits=False, label_smoothing=0.0, axis=-1, name=\"binary_crossentropy\"\n",
    ")\n",
    "\n",
    "\n",
    "#####\n",
    "# Workflow\n",
    "#####\n",
    "def initialize_and_compile_model(optimizer, lossfn):\n",
    "    \"\"\"\n",
    "    Initializes the model, compiles it with the specified optimizer and loss function,\n",
    "    and prints the model summary.\n",
    "\n",
    "    Parameters:\n",
    "    - optimizer: The optimizer to use for training the model.\n",
    "    - lossfn: The loss function to use for training.\n",
    "\n",
    "    Returns:\n",
    "    - Compiled TensorFlow model.\n",
    "    \"\"\"\n",
    "    print(\"\\nInit the model :\")\n",
    "    model = initialize_model()\n",
    "    model.compile(optimizer=optimizer, loss=lossfn, metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "def batch_dataset(dataset, batch_size: int):\n",
    "    \"\"\"\n",
    "    Batches the dataset with the specified batch size.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset: The dataset to batch.\n",
    "    - batch_size: The size of each batch.\n",
    "\n",
    "    Returns:\n",
    "    - Batched dataset.\n",
    "    \"\"\"\n",
    "    return dataset.batch(batch_size)\n",
    "\n",
    "\n",
    "def split_dataset(batched_dataset, ratio: float = 0.8):\n",
    "    \"\"\"\n",
    "    Splits the batched dataset into training and testing datasets.\n",
    "\n",
    "    Parameters:\n",
    "    - batched_dataset: The batched dataset to split.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple containing the training and testing datasets.\n",
    "    \"\"\"\n",
    "    size = int(len(batched_dataset) * ratio)\n",
    "\n",
    "    train = batched_dataset.take(size)\n",
    "    test = batched_dataset.skip(size)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def train_model(model, train, test, epochs: int, callbacks: list):\n",
    "    \"\"\"\n",
    "    Trains the model on the training dataset and validates it on the testing dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The model to train.\n",
    "    - train: The training dataset.\n",
    "    - test: The testing dataset.\n",
    "    - epochs: The number of epochs to train for.\n",
    "    - callbacks: A list of callbacks to use during training.\n",
    "\n",
    "    Returns:\n",
    "    - History object resulting from model training.\n",
    "    \"\"\"\n",
    "    history = model.fit(\n",
    "        train,\n",
    "        validation_data=test,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "    return history\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Upload alls jpeg to create the test\n",
    "#     # upload_files_to_gcp(BUCKET_NAME, RAW_DATA)\n",
    "\n",
    "#     # Walk inside the bucket & create the metadata\n",
    "#     # create_and_upload_merged_csv(BUCKET_NAME, METADATA)\n",
    "\n",
    "    # print(\"==== Starting Workflow ====\")\n",
    "\n",
    "    # # Step 1: Create the Dataset\n",
    "    # print(\"\\n=== Step 1: Creating the Dataset ===\")\n",
    "    # dataset = create_dataset()\n",
    "    # print(\"Dataset created successfully.\")\n",
    "    # print(dataset)\n",
    "\n",
    "    # # Step 2: Initialize and Compile the Model\n",
    "    # print(\"\\n=== Step 2: Initializing and Compiling the Model ===\")\n",
    "    # model = initialize_and_compile_model(optimizer, lossfn)\n",
    "    # print(\"Model initialized and compiled successfully.\")\n",
    "\n",
    "    # # Step 3: Batch the Dataset\n",
    "    # print(\"\\n=== Step 3: Batching the Dataset ===\")\n",
    "    # batched_dataset = batch_dataset(dataset, BATCH_SIZE)\n",
    "    # print(f\"Dataset batched with batch size {BATCH_SIZE}.\")\n",
    "\n",
    "    # # Step 4: Create Train/Test Split\n",
    "    # print(\"\\n=== Step 4: Creating Train/Test Split ===\")\n",
    "    # train, test = split_dataset(batched_dataset, 0.8)\n",
    "    # print(\n",
    "    #     f\"Train/Test split created. Train size: {len(train)}, Test size: {len(test)}.\"\n",
    "    # )\n",
    "\n",
    "    # # Step 5: Train the Model\n",
    "    # print(\"\\n=== Step 5: Training the Model ===\")\n",
    "    # history = train_model(model, train, test, epochs=5, callbacks=[custom_callback])\n",
    "    # print(\"Model training complete.\")\n",
    "\n",
    "    # # Conclusion\n",
    "    # print(\"\\n==== Workflow Completed Successfully ====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Starting Workflow ====\n",
      "\n",
      "=== Step 1: Creating the Dataset ===\n",
      "Dataset created successfully.\n",
      "<_MapDataset element_spec=(TensorSpec(shape=(128, 128, 1), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.int32, name=None))>\n",
      "\n",
      "=== Step 2: Initializing and Compiling the Model ===\n",
      "\n",
      "Init the model :\n",
      "Model initialized and compiled successfully.\n",
      "\n",
      "=== Step 3: Batching the Dataset ===\n",
      "Dataset batched with batch size 64.\n",
      "\n",
      "=== Step 4: Creating Train/Test Split ===\n",
      "Train/Test split created. Train size: 684, Test size: 171.\n",
      "\n",
      "=== Step 5: Training the Model ===\n",
      "Epoch 1/5\n",
      "212/684 [========>.....................] - ETA: 53:29 - loss: 0.6772 - accuracy: 0.7965"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-11 16:40:16.643883: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at whole_file_read_ops.cc:116 : FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: storage.googleapis.com\n",
      "\t when reading gs://mammo_data/1472828712.jpg\n"
     ]
    },
    {
     "ename": "FailedPreconditionError",
     "evalue": "Graph execution error:\n\nDetected at node ReadFile defined at (most recent call last):\n<stack traces unavailable>\nError executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: storage.googleapis.com\n\t when reading gs://mammo_data/1472828712.jpg\n\t [[{{node ReadFile}}]]\n\t [[IteratorGetNext]] [Op:__inference_train_function_1016]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Step 5: Train the Model\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Step 5: Training the Model ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcustom_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel training complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Conclusion\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 232\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train, test, epochs, callbacks)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(model, train, test, epochs: \u001b[38;5;28mint\u001b[39m, callbacks: \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    219\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;124;03m    Trains the model on the training dataset and validates it on the testing dataset.\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;124;03m    - History object resulting from model training.\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m history\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/mammo-analysis/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/mammo-analysis/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Graph execution error:\n\nDetected at node ReadFile defined at (most recent call last):\n<stack traces unavailable>\nError executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: storage.googleapis.com\n\t when reading gs://mammo_data/1472828712.jpg\n\t [[{{node ReadFile}}]]\n\t [[IteratorGetNext]] [Op:__inference_train_function_1016]"
     ]
    }
   ],
   "source": [
    "print(\"==== Starting Workflow ====\")\n",
    "\n",
    "# Step 1: Create the Dataset\n",
    "print(\"\\n=== Step 1: Creating the Dataset ===\")\n",
    "dataset = create_dataset(input='cloud')\n",
    "print(\"Dataset created successfully.\")\n",
    "print(dataset)\n",
    "\n",
    "# Step 2: Initialize and Compile the Model\n",
    "print(\"\\n=== Step 2: Initializing and Compiling the Model ===\")\n",
    "model = initialize_and_compile_model(optimizer, lossfn)\n",
    "print(\"Model initialized and compiled successfully.\")\n",
    "\n",
    "# Step 3: Batch the Dataset\n",
    "print(\"\\n=== Step 3: Batching the Dataset ===\")\n",
    "batched_dataset = batch_dataset(dataset, BATCH_SIZE)\n",
    "print(f\"Dataset batched with batch size {BATCH_SIZE}.\")\n",
    "\n",
    "# Step 4: Create Train/Test Split\n",
    "print(\"\\n=== Step 4: Creating Train/Test Split ===\")\n",
    "train, test = split_dataset(batched_dataset, 0.8)\n",
    "print(\n",
    "    f\"Train/Test split created. Train size: {len(train)}, Test size: {len(test)}.\"\n",
    ")\n",
    "\n",
    "# Step 5: Train the Model\n",
    "print(\"\\n=== Step 5: Training the Model ===\")\n",
    "history = train_model(model, train, test, epochs=5, callbacks=[custom_callback])\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# Conclusion\n",
    "print(\"\\n==== Workflow Completed Successfully ====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mammo-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
